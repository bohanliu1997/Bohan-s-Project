########################################################
### Case: 2008 Democratic Primaries - Clinton vs. Obama
########################################################
source("DataAnalyticsFunctions.R")
# read data into R
election_data <- read.csv("ElectionDataAlone.csv")

# Next use the function summary to inspect the data
summary(election_data)

##############################################
# Cleaning up the data
# Write a function that replaces NAs with the mean of the non-missing data 
# in the column. This function can be called for different data sets to 
# impute the data.
impute_data <- function(vec, mn) {
  ifelse(is.na(vec), mn, vec)
}
# Find the means for all the numeric columns. 
# The function sapply automatically runs the mean function 
# (specified as second argument) on the columns 10 through 41. 
# The means are then saved in the vector named train_data_mean. 
# We use the argument na.rm=TRUE to ask the function to ignore NA entries.
data_mean <- sapply(election_data[,10:41],mean, na.rm=TRUE)

# Run this command to look at means for all the columns we found by running the sapply function
(data_mean)

# Impute the missing data. Loop through all the rows and 
# for each column call the function impute_train_data.
for(i in 10:41) {
  election_data[,i]<-impute_data(election_data[,i],data_mean[i-9])
}
# Run the summary function again. Now you see that no demographic/county columns have NA entries.
summary(election_data)

# Create two separate data sets from the data in electionData.
election_data$ElectionDate <- as.Date(election_data$ElectionDate, format="%m/%d/%Y")
election_data_train <- election_data[election_data$ElectionDate < as.Date("2/19/2008", format="%m/%d/%Y"), ]
election_data_test <- election_data[election_data$ElectionDate >= as.Date("2/19/2008", format="%m/%d/%Y"), ]

# If you want to write these data sets back out into spreadsheets, 
# use the following "write" commands in R.
# write.csv(electionDataTrain, "electionDataTrain.csv")
# write.csv(electionDataTest, "electionDataTest.csv")

##########################################################
### End of Data Cleaning up
##########################################################
#
# Create some possible variables that might be of interest.
# (things we might like to predict in a regression using the demographic information). 
# These variables directly become a part of our data set election_data_train. You can use the command names(election_data_train) to see that these are added as columns in our data set.
election_data_train$Obama_margin <- election_data_train$Obama - election_data_train$Clinton
election_data_train$Obama_margin_percent <- 100*election_data_train$Obama_margin/election_data_train$TotalVote
election_data_train$Obama_wins <- ifelse(election_data_train$Obama_margin >0, 1,0)
names(election_data_train)
###
### Based on the data, to account for the size of possible delegates on each county
### we will work with election_data_train$Obama_margin_percent to be the target out models.
###

###
### Question 1: Provide a visualization (very flexible format, 
### it does not need to be related to the election)
### 
fin.df<-subset(election_data, select=c("AgeBelow35", "Age35to65", "Age65andAbove", "White", "Black", 
                                       "Asian", "Hispanic","HighSchool", "Bachelors", "Poverty", "IncomeAbove75K", "MedianIncome", "AverageIncome", 
                                       "UnemployRate", "SpeakingNonEnglish", "Medicare", "MedicareRate", 
                                       "SocialSecurityRate", "RetiredWorkers","Disabilities", 
                                       "DisabilitiesRate", "Homeowner", "SameHouse1995and2000","Pop"))
CorMatrix <- cor(fin.df)
corrplot(CorMatrix, method = "square")

library(ggplot2)

ggplot(data=election_data, aes(x=log(TotalVote), y=DisabilitiesRate))+
  geom_point()

ggplot(data=election_data,aes(x=Region, y=TotalVote/Pop, fill=Region))+geom_boxplot()+ylim(0,0.5)+
  ylab(label = "Percent")+ggtitle("Percentage of votes per region")+ theme(plot.title = element_text(hjust = 0.5))
###
### Question 2: Prediction. No additional script beyond the cleaning up the data
### provided above. (Hint: FIPS variable bahaves as an identifier for each observation
### so you might not want to include it in a linear regression.)
###

###
### Question 3: Keep in mind that unsupervised learning 
### is used to explore the data. Feel free to consider only a subset of the 
### demographic variables. 

#Creating data frame for the particular columns to cluster on
county <- election_data[, c(11,12,13,15,16,17,20, 21, 22, 23, 24, 25, 27, 29, 31, 33, 36, 37, 38, 40)]

#Scaling the columns to standardize the values
scale_county <- scale(county)
scale_county <- data.frame(scale_county)

#Checking to ensure standardization is appropriately performed. Standard deviation = 1 and mean is nearly 0 so our data is ready to be clustered.
apply(scale_county, 2, sd)
apply(scale_county, 2, mean)

#Identifying the appropriate number of clusters:
#We first obtain the AIC and BIC values for all models ranging from 1 to 100 clusters.
kfit <- lapply(1:100, function(k) kmeans(scale_county,k))
kfit
kaicc <- sapply(kfit,kIC)
kbic  <- sapply(kfit,kIC,"B")
## Plotting AIC and BIC
par(mar=c(1,1,1,1))
par(mai=c(1,1,1,1))
plot(kaicc, xlab="K", ylab="IC", 
     ylim=range(c(kaicc,kbic)), # get them on same page
     type="l", lwd=2)
abline(v=which.min(kaicc))
# Next we plot BIC
lines(kbic, col=4, lwd=2)
# Vertical line where BIC is minimized
abline(v=which.min(kbic),col=4)

k=37
fit <- 1 - sum(kfit[[k]]$tot.withinss)/kfit[[k]]$totss
paste("The goodness of fit of our clustering model is ", fit)

#Generating the cluster model with 37 clusters
countyclusters <- kmeans(scale_county,37,nstart=10)
countyclusters$cluster
#We can see that our clusters are of widely different sizes
countyclusters$size
#And here are their centers
countyclusters$centers
#We can combine our main election_data data frame with our clusters to help us create interesting visualizations
election_data <- cbind(election_data, countyclusters$cluster)
ggplot(election_data, aes(x=AgeBelow35, y=Poverty, color=factor(countyclusters$cluster))) + geom_point() + ylab("Poverty Rate (%)") + xlab("Proportion of Population Under 35") + theme(legend.position = "none") + xlim(20,80)
ggplot(election_data, aes(x=HighSchool, y=UnemployRate, color=factor(countyclusters$cluster))) + geom_point() + xlim(50,100) + ylim(2.5,13) + theme(legend.position = "none") + xlab("Proportion of High School Graduates") + ylab("Unemployment Rate")


###
### Question 4(a) impact of changing hispanic demographic
###
library(glmnet)
#### Model with 1771 controls to measure the impact of 10% larger Hispanic demographic
y <- election_data_train$Obama_margin_percent
x <- model.matrix( Obama_margin_percent ~ .-Hispanic-Obama_wins-Obama_margin-FIPS-ElectionDate-TotalVote-Clinton-Obama, data = election_data_train )
d <- election_data_train$Hispanic
####
#### Feel free to compare/contrast your results with the following simple regression model
#### 
HispanicSimple <- glm( Obama_margin_percent ~ Hispanic, data = election_data_train )
summary(HispanicSimple)

#### Double Selection
num.features <- ncol(x)
num.n <- nrow(x)
### Step 1
## we set penalty level using the theoretical choice
w <-sd(y)
lambda.theory <- 2*w*sqrt(log(num.features/0.01)/num.n)
## call Lasso 
lassoTheory <- glmnet(x,y,lambda = lambda.theory)
## get the support
supp1 <- support(lassoTheory$beta)
### Step 1 selected
length(supp1)
colnames(x[,supp1])
### controls
###
### Step 2
w <-sd(d)
lambda.theory <- 2*w*sqrt(log(num.features/0.05)/num.n)
lassoTheory <- glmnet(x,d,lambda = lambda.theory)
supp2<-support(lassoTheory$beta)
### Step 2 selected
length(supp2)
### controls
colnames(x[,supp2])
###
### Step 3
inthemodel <- unique(c(supp1,supp2)) # unique grabs union
selectdata <- cbind(d,x[,inthemodel]) 
selectdata <- as.data.frame(as.matrix(selectdata)) # make it a data.frame
dim(selectdata) ## p about half n

## run a a linear regression of Y on d and all selected
causal_glm <- glm(y~., data=selectdata)
## The theory actually says that the standard SE calc for gamma is correct!
## despite the model selection
summary(causal_glm)$coef["d",]
####
### Question 4(b) impact of changing black demographic
####
#### Model with 1771 controls to measure the impact of 10% larger Black demographic
y <- election_data_train$Obama_margin_percent
x <- model.matrix(Obama_margin_percent ~ .-Black-Obama_wins-Obama_margin-FIPS-ElectionDate-TotalVote-Clinton-Obama, data = election_data_train )
d <- election_data_train$Black
####
#### Feel free to compare/contrast your results with the following simple regression model
#### 
num.features <- ncol(x)
num.n <- nrow(x)
### Step 1
## we set penalty level using the theoretical choice
w <-sd(y)
lambda.theory <- 2*w*sqrt(log(num.features/0.01)/num.n)
## call Lasso 
lassoTheory <- glmnet(x,y,lambda = lambda.theory)
## get the support
supp1 <- support(lassoTheory$beta)
### Step 1 selected
length(supp1)
colnames(x[,supp1])
### controls
###
### Step 2
w <-sd(d)
lambda.theory <- 2*w*sqrt(log(num.features/0.05)/num.n)
lassoTheory <- glmnet(x,d,lambda = lambda.theory)
supp2<-support(lassoTheory$beta)
### Step 2 selected
length(supp2)
### controls
colnames(x[,supp2])
###
### Step 3
inthemodel <- unique(c(supp1,supp2)) # unique grabs union
selectdata <- cbind(d,x[,inthemodel]) 
selectdata <- as.data.frame(as.matrix(selectdata)) # make it a data.frame
dim(selectdata) ## p about half n

## run a a linear regression of Y on d and all selected
causal_glm <- glm(y~., data=selectdata)
## The theory actually says that the standard SE calc for gamma is correct!
## despite the model selection
summary(causal_glm)$coef["d",]

####


####
#### Question 5: No additional R code. Keep in mind that you can build upon your previous 
#### analysis or develop new analysis.
####

